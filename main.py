import os
import streamlit as st
import torch
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langchain_community.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
import numpy as np

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± @Mineru
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")
# =============================
# .bashrc íŒŒì¼ì— 
# export HF_HOME="./.cache/" << ì¶”ê°€
# =============================

RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë‹µë³€ì— ìì„¸í•˜ê²Œ ëŒ€ë‹µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ëª¨ë¥´ëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. ë³µì¡í•œ ì‘ì—…ì„ ë” ê°„ë‹¨í•œ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ë‹¨ê³„ì—ì„œ "ìƒê°"í•  ì‹œê°„ì„ ê°€ì§€ì„¸ìš”. ê·¸ë¦¬ê³  ë‹µë³€ ëì— ì°¸ê³ í•œ ë¬¸ì„œë¥¼ í‘œê¸°í•˜ì‹­ì‹œì˜¤.
ì•„ë˜ëŠ” ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ì˜ˆì œ ë‹µë³€ì…ë‹ˆë‹¤.

Question: ì¥í•™ìƒ ì„ ë°œ ê¸°ì¤€ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜
Context: ì¥í•™ìƒ ì„ ë°œ ê¸°ì¤€ì€ ì„±ì , ë¦¬ë”ì‹­, ì‚¬íšŒë´‰ì‚¬, ì¬ì • ìƒíƒœ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ê²°ì •ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•™ì—… ì„±ì·¨ë„ì™€ ë¦¬ë”ì‹­ì´ ì£¼ìš” í‰ê°€ ìš”ì†Œì…ë‹ˆë‹¤.
Answer: ì¥í•™ìƒ ì„ ë°œ ê¸°ì¤€ì€ ì£¼ë¡œ ì„±ì ê³¼ ë¦¬ë”ì‹­ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì„±ì ì€ í•™ì—… ì„±ì·¨ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë¦¬ë”ì‹­ì€ í•™ìƒì´ ê³µë™ì²´ì—ì„œ ì–´ë– í•œ ì—­í• ì„ ìˆ˜í–‰í–ˆëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ë˜í•œ, ì‚¬íšŒë´‰ì‚¬ì™€ ì¬ì • ìƒíƒœë„ ê³ ë ¤ë©ë‹ˆë‹¤. ì´ ëª¨ë“  ìš”ì†Œë“¤ì´ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€ë˜ì–´ ì¥í•™ìƒì´ ì„ ë°œë©ë‹ˆë‹¤.
ì¶œì²˜: ì¥í•™ê·œì • ë¬¸ì„œ

Question: ì¥í•™ìƒì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì¥í•™ì¢…ë¥˜ëŠ” ì£¼ë¡œ ìˆì–´?
Context: ì¥í•™ìƒì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ë¶„í•˜ê³ , ì¥í•™ìƒ êµ¬ë¶„ì— ë”°ë¥¸ ì¥í•™ì¢…ë¥˜ëŠ” ë³„í‘œ 1ê³¼ ê°™ì´ í•œë‹¤. 1. ëª…ì˜ˆì¥í•™ìƒ 2. ì„±ì ìš°ìˆ˜ì¥í•™ìƒ 3. íŠ¹ë³„ì¥í•™ìƒ 4. êµì™¸ì¥í•™ìƒ 5. êµ­ê°€ì¥í•™ìƒ
Answer: 1. ëª…ì˜ˆì¥í•™ìƒ 2. ì„±ì ìš°ìˆ˜ì¥í•™ìƒ 3. íŠ¹ë³„ì¥í•™ìƒ 4. êµì™¸ì¥í•™ìƒ 5. êµ­ê°€ì¥í•™ìƒ ì´ 5ê°€ì§€ë¡œ ì¥í•™ì¢…ë¥˜ë¥¼ êµ¬ë¶„í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì¶œì²˜: ì¥í•™ê·œì • ë¬¸ì„œ

Question: {question}
Context: {context}
Answer:
ì¶œì²˜: {context}"""

st.set_page_config(page_title="RAGë¥¼ ì´ìš©í•œ ì±—ë´‡", page_icon="ğŸ’¬")
st.title("RAGë¥¼ ì´ìš©í•œ ì±—ë´‡")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)

# ì…ë ¥ë°›ì€ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ëŠ” ê³¼ì •
@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    
    tik_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        encoding_name="cl100k_base",
        chunk_size=250,
        chunk_overlap=0
    )
    
    # ë‹¤ì–‘í•œ ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê¸° ìœ„í•´ì„œ unstructuredFileLoaderë¥¼ ì‚¬ìš©
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=tik_text_splitter)

    # ëª¨ë¸ ë° ì„ë² ë”© ì„¤ì •
    # gpu ê°€ì†ì„ ìœ„í•œ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # ë‚´ê°€ ë”°ë¡œ ì„¤ì •í•œ ì„ë² ë”© ëª¨ë¸
    model_name = "intfloat/multilingual-e5-large-instruct"
    model_kwargs = {'device': device}
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    # VectorDBë¡œëŠ” FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì„±í•˜ì˜€ìŒ
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    retriever = vectorstore.as_retriever(k=3)
    
    return retriever, vectorstore, embeddings  # vectorstoreì™€ embeddings ë°˜í™˜

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)

with st.sidebar:
    file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever, vectorstore, embeddings = embed_file(file)

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        # ngrok remote ì£¼ì†Œ ì„¤ì •
        ollama = ChatOllama(model="EEVE-Korean-10.8B:latest")
        chat_container = st.empty()
        if file is not None:
            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | ollama
                | StrOutputParser()
            )
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            answer = rag_chain.stream(user_input)
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
        else:
            prompt = ChatPromptTemplate.from_template(
                "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
            )

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            chain = prompt | ollama | StrOutputParser()

            answer = chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
